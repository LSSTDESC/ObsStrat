% Need these new commands to compile:
%\newcommand{\todo}[2]{\textcolor{red}{\textbf{TODO (#1): #2}}}
%\newcommand{\comment}[1]{\textcolor{blue}{\textbf{#1}}}


\clearpage

\section{Strong Lensing}
\textit{Authors: Simon Huber\footnote{shuber@mpa-garching.mpg.de}, Sherry H.~Suyu\footnote{suyu@mpa-garching.mpg.de}, Tanja Petrushevska\footnote{tanja.petrushevska@ung.si} }

\

\subsection{Supernovae Lensed by Galaxies}
\textit{Contributors: Simon Huber, Sherry H.~Suyu}

The goal of this section is to evaluate how well we can measure time delays in strongly lensed SNe Ia (LSNe Ia), which yields with lens mass modeling a direct method to measure the Hubble constant. 
 
To simulate observations randomly, we have used
mock LSNe Ia from the OM 10 catalog \citep{Oguri:2010},
and produced the light curves for the mock SNe images with
the spherically symmetric SN Ia W7 model \citep{1984:Nomoto}
calculated with ARTIS (Applied Radiative Transfer In Supernovae)
\citep{Kromer:2009ce} in combination with magnifications maps from
GERLUMPH \citep{Vernardos:2015wta} to include the effect of
microlensing similar as in \citep{Goldstein:2017bny}. To simulate
data points for the light curves, we place the mock systems randomly in one of 10 chosen
fields in the WFD (wide fast deep) survey, where we follow the observation pattern from 20 different cadence strategies.
From this we get simulated observations, where the error is calculated according to \cite[sec 3.5,
p. 67]{2009:LSSTscience}.
To evaluate the mock data and get a measured time delay we use the
free knot spline optimizer from PyCS (Python Curve Shifting)
\citep{2013:Tewesb,Bonvin:2015jia}. Details of this
work will be presented in Huber et al. (in preparation).

We have investigated two different metrics, first using LSST data only to measure time delays and second, using LSST as a discovering machine in combination with follow-up observations to measure delays. We assume follow-up observations would start 2 days after the third LSST data point in any filter exceeds the 5-$\sigma$ depth, where the follow-up is done in 3 filters (g, r, i) every second night. 

To have sufficient statistics, we investigate for each cadence
strategy 202 mock LSNe Ia for "LSST only", and 100 mock systems for "LSST + follow up". For each of the mock systems we draw 100 random starting
configurations. A starting configuration corresponds to a random
position in the microlensing map and a random field of the 10 chosen fields, where it is placed randomly in one of the
observing seasons such that the detection requirement of OM 10 is
fulfilled. For each of these starting configurations we then draw 1000
different noise realizations of light curves. For each realization we calculate the deviation from the true time delay as
%
\begin{equation}
\tau_\mathrm{d} = \frac{t_\mathrm{measured} - t_\mathrm{true}}{t_\mathrm{true}}.
\label{eq: deviation from true time delay}
\end{equation}
For one strategy and double LSNe Ia, we have thus $1 {\rm (delay\ for\ the\
one\ pair\ of\ images)} \times 6 {\rm (filters)} \times 100 {\rm
(starting\ configurations)} \times 1000 {\rm (noise\ realisations)}$
time-delay deviations as in \eqref{eq: deviation from true time delay}.
%, where the 6 stands for the 6 LSST filters. 
For the 6 pairs of images for a quad system we have a sample of $6
\times 6 \times 100 \times 1000$. The resulting distribution of
time-delay deviation is investigated for each pair of images and each
filter separately. From the $100 \times 1000$ time-delay deviations we
define accuracy as the median $\tau_\mathrm{d,50}$ and precision as
$\delta = (\tau_\mathrm{d,84}-\tau_\mathrm{d,16})/2$, where
$\tau_\mathrm{d,84}$ is the 84th and $\tau_\mathrm{d,16}$ the 16th
percentile. Measuring $H_0$ with 1\% accuracy requires that the accuracy
in the delay deviation $\tau_\mathrm{d,50}$ is $<1\%$ (since $H_0 \propto
t_{\rm true}^{-1}$). Since the 6 time-delay deviations from the 6 filters are independent we combine them into a single time-delay deviation via the weighted mean. This means that in the end we have for one strategy and a mock LSNe Ia one 
\begin{equation}
\tau_\mathrm{d,50} \pm \delta
\label{eq: accuracy and precission}
\end{equation}
per pair of images.

From this we summarize the results and quantify the 20 investigated cadences. Given that $H_0 \propto \frac{1}{t}$, where $t$
is the time delay between two images, we aim for accuracy
($\tau_\mathrm{d,50}$) smaller than 1 percent and precision ($\delta$)
smaller than 5 percent in equation \ref{eq: accuracy and precission}. The accuracy
requirement is needed for measuring $H_0$ with 1\% uncertainty, and
the precision requirement ensures that the delay uncertainty does not
dominate the overall uncertainty on $H_0$ given typical mass modeling
uncertainties of $\sim 5\%$ \citep[e.g.,][]{Suyu2018}.  A quad system is counted as successful if one of the 6 delays fulfills this requirement. 

From this investigation we get the fraction of systems with successful measured delays. These numbers have to be combined with the total number of LSNe Ia we expect to detect for different strategies. We approximate the total number of LSNe Ia as

\begin{align}
\label{eq: total number of LSNe Ia from modified OM 10}
N_\mathrm{LSNe Ia, cad} = N_\mathrm{LSNe Ia, OM 10} \frac{\Omega_\mathrm{cad}}{\Omega_\mathrm{OM 10}} \frac{\bar{t}_\mathrm{eff,cad}}{t_\mathrm{eff, OM 10}}
\end{align}
%
where $N_\mathrm{LSNe Ia, OM 10} = 45.7$, $\Omega_\mathrm{OM 10} = \SI{20000}{\square\deg}$ and $t_\mathrm{eff, OM 10}=\SI{2.5}{\year}$ from \cite{Oguri:2010}. $\Omega_\mathrm{cad}$ is the survey area for a given cadence and we calculate it by taking all fields into account with a cumulative season length, inter-night gap and 5-sigma depth similar to a WFD sub-sample\footnote{The sub-sample contains 719 WFD fields. The calculated areas are for most strategies within a few percent of the nominal values ($\Omega_\mathrm{cad}=\SI{18000}{\square\deg}$ or $\Omega_\mathrm{cad}=\SI{24700}{\square\deg}$). The maximum deviation is $17 \%$ for nexus\_2097. Details will be presented in Huber et al. (in preparation).}. $\bar{t}_\mathrm{eff,cad}$ is the cumulative seasonal length for a given cadence, where we have averaged over the WFD sub-sample. 

Combining the fraction of systems with the total number of LSNe Ia we get the number of LSNe Ia over the 10 year survey, where the delay can be measured with accuracy $<$ 1 \% and precision $<$ 5 \% . The results are shown in Figure \ref{fig:final results}. For a better interpretation we subdivide the strategies into three categories: (1) \textcolor{orange} {baseline like in terms of inter-night gap and cumulative season length} (mean values), (2) \textcolor{blue}{shorter inter-night gap, shorter cumulative season length} and (3) \textcolor{magenta}{shorter inter-night gap, baseline like cumulative season length}. In addition cadences with a large nominal WFD footprint start with a capital letter.

%
\begin{figure}[h!]
\centering
\includegraphics[scale=1]{sl_MetricLSST.pdf}
\caption{This plot quantifies different cadence strategies for measuring time delays in LSNe Ia. The black bars consider LSST as a discovery machine in combination with follow-up observations in 3 filters (g,r,i) every second night. We assume follow-up observation would start 2 days after the third LSST data point in any filter exceeding the 5-$\sigma$ depth. For the grey bars only LSST data is used to measure time delays. Since for LSST only, the total numbers with good delays over the 10 year survey are very low, we advocate using LSST as a discovering machine with observational follow up. Therefore the black bars are the relevant ones to quantify different cadences.}
\label{fig:final results}
\end{figure}

For the metric LSST data only we see that for the best strategies we will just have
a few systems where time-delay measurements are possible. Follow-up observations are therefore necessary to
increase the number of LSNe Ia with delays which makes LSST + follow up the important metric to rank different observing strategies.

To summarize, for our science case of measuring time delays from as many lensed SNe as possible, it would be more effective to use LSST as a discovering machine with additional follow-up, instead of relying on LSST completely for the delay measurements. From our investigations we find that long cumulative seasonal lengths $\bar{t}_\mathrm{eff,cad}$ and a more frequent sampling are important. To go more into detail, we request for 10 seasons with a season length of 170 days or longer. Rolling cadences are clearly disfavored, because their shortened cumulative season lengths lead to overall a more negative impact on the number of LSNe Ia with delays, compared to the gain from the increased sampling frequency\footnote{rolling\_mix\_10\_yrs\_opsim has a cumulative season length close to baseline2018a and does the revisit in different filters. It is therefore the only rolling cadence performing better than the baseline cadence.}. To improve the sampling, single visits per night would be helpful. Since this will make the science case of fast-moving transients impossible we suggest doing one revisit within a night but in a different band than the first visit. Further improvements are the replacement of $2 \times \SI{15}{\s}$ exposure by $1 \times \SI{30}{\s}$ for an increased efficiency and redistributing some of the visits in y band to g, r, i and z.
%of their shorter cumulative season lengths $\bar{t}_\mathrm{eff,cad}$ although they improve the sampling. Therefore we reject to improve on one of the 3 parameters our science case is mostly sensitive to, by worsen at the same time one of the others significantly. 
%

Further \cite{Goldstein:2018bue} performed detailed simulations of the gLSN population using a completely independent technique and pipeline and reached similar conclusions to the ones presented here: rolling cadences are strongly disfavored, and wide-area, long-season surveys with well sampled light curves are optimal.



\FloatBarrier
\subsection{Supernovae Lensed by Galaxy Clusters}
\textit{Contributors: Tanja Petrushevska}

Here, we focus on prospects of observing supernovae which are lensed by known galaxy clusters. High-z galaxies that appear as multiple images in the cluster field can host supernova explosions \citep{2015Sci...347.1123K, 2016ApJ...819L...8K}. Strongly lensed supernovae by galaxy clusters not only can be used as tools to examine both global cosmology, but also the local environment of the cluster lenses \citep{2014ApJ...786....9P,2014MNRAS.440.2742N, 2015ApJ...811...70R}. Cluster lensing time scales are typically much longer and the microlensing effects are almost negligible, which makes their measurement potentially more feasible, especially if the lens potential is well studied and the predicted time delays have small uncertainties. We calculate the expected number of supernovae Ia in the multiply lensed background galaxies by using the six Hubble Frontier Fields clusters \citep{2017ApJ...837...97L}  (Abell 2744, MACS J0416.1-2403, MACS J0717.5+3745, MACS J1149.5+2223, Abell S1063 and Abell 370) and Abell 1689. These clusters have been extensively studied, and given the good quality data, well constrained magnification maps and time delays can be obtained from the lensing models (\cite{2016A&A...594A..54P, 2018A&A...614A.103P}, Petrushevska et al. 2018, in press). We only considered the multiply imaged galaxies that have a spectroscopic redshift. To obtain better image depth, we combine the images that are taken closer than 5 days in time. We note that these are a lower limits, since we have only considered few clusters and the galaxies with spectroscopic redshift. For this science case, the most important bands are i, z and y. Since most of the light of nearby SNe is in the optical bands, these filters are optimal for finding high-z SNe, as their light is redshifted to the longer wavelengths. For the details regarding the estimates presented in this section, we refer to our work (\cite{2018A&A...614A.103P,2016A&A...594A..54P}, Petrushevska et al. 2018, in press). The results are presented in Figure \ref{?}. We confirm independently the conclusions from the previous section. Furthermore, our general recommendation would be, in order to optimise the sensitivity to high-redshift SNe with multiple images in galaxy cluster fields, to obtain better image depth in the reddest bands (i, z and y) . This can also be obtained by co-adding images from visits closely separated in time.

\begin{figure}
\centering
\includegraphics[scale=0.65]{figures/galaxy_lensing.pdf}
\caption{The expected total number of strongly lensed SNe Ia arising from the multiply imaged galaxies in the Hubble Frontier Fields and Abell 1689 in function of the observing strategy. \todo{Tanja}{include new cadences and do same estimates for CC SNe*}}
\end{figure}


\subsection{Lensed Quasars}
\textit{Contributors: TBD}



\FloatBarrier


