\section{Weak Lensing Systematics}

The LSST provides an opportunity to mitigate WL systematics using the observing strategy. This opportunity was not possible in previous surveys because the LSST will be the first survey to dither at large scales (relative to the field of view) with a large number of exposures.

Weak lensing systematics such as PSF modeling errors have been observed to have specific directions in single exposures. With a suitable dithering algorithm, one can average down these systematics. While the process of averaging them down is more complicated, it strongly correlates with the number of observations for a set of simulated, uniformly distributed objects on the sky, restricted to i-band only (which the main weak lensing analysis will use). We, therefore, use the average number of i-band visits per object as our metric.

It is essential for these visits to be of high quality. There are already several metrics in MAF that ensure this, the ones that specifically apply are coadded depth (ExgalM5 or CoaddM5), the airmass distribution, and the seeing distribution. More uniform strategies will have narrow distributions. The reason these metrics provide similar results to our analysis is that survey uniformity is one of the crucial things that average down WL systematics, and all these metrics are, in one way or another, a measure of uniformity.

Figure~\ref{fig:WLSystematicsRankings} shows the result of this metric for the 16 recent OpSim runs. 

Certain trends can be seen from  figure~\ref{fig:WLSystematicsRankings}: more visits in the main WFD survey are very beneficial for averaging down weak lensing systematics (whether achieved by smaller area, shorter 20-second visits, or single 30-second exposures that reduces total readout time. OpSim runs with more exposure to the galactic planes underperform due to high extinction and inferior observing conditions in the galactic plane makes it unusable for weak lensing analysis. Runs with rolling cadence significantly underperform others, especially ones that have a rolling cadence in all or most years. Rolling cadence runs have to go back to uniformity at times of result releases, but otherwise, this underperformance has mostly been explained by bugs in the OpSim runs. We generally cannot comment on rolling cadence runs until we have analyzed runs without any bugs. Finally, wider area OpSim runs also do worse because using the same amount of time to cover a larger area will decrease the overall depth.

\begin{figure}[htb]
    \centering
    \caption{Placeholder, will add figure soon.}
    \label{fig:WLSystematicsRankings}
\includegraphics[\textwidth]{}
\end{figure}
