% Need these new commands to compile:
%\newcommand{\todo}[2]{\textcolor{red}{\textbf{TODO (#1): #2}}}
%\newcommand{\comment}[1]{\textcolor{blue}{\textbf{#1}}}


\newcommand{\altsched}{\textcolor{magenta}{alt\_sched}} 
\newcommand{\altschedrolling}{\textcolor{blue}{alt\_sched\_rolling}}
\newcommand{\baseline}{\textcolor{orange}{baseline2018a}}
\newcommand{\colossusfour}{\textcolor{orange}{colossus\_2664}}
\newcommand{\colossusfive}{\textcolor{orange}{colossus\_2665}}
\newcommand{\colossusseven}{\textcolor{magenta}{colossus\_2667}}
\newcommand{\krakentwosix}{\textcolor{orange}{kraken\_2026}}
\newcommand{\krakenfive}{\textcolor{orange}{kraken\_2035}}
\newcommand{\krakenthreesix}{\textcolor{blue}{kraken\_2036}}
\newcommand{\krakentwo}{\textcolor{orange}{kraken\_2042}}
\newcommand{\krakenfour}{\textcolor{magenta}{Kraken\_2044}}
\newcommand{\mothrafive}{\textcolor{blue}{mothra\_2045}}
\newcommand{\mothranine}{\textcolor{blue}{Mothra\_2049}}
\newcommand{\nexusseven}{\textcolor{blue}{Nexus\_2097}}
\newcommand{\pontuszerozerotwo}{\textcolor{orange}{Pontus\_2002}}
\newcommand{\pontusfivezerotwo}{\textcolor{orange}{pontus\_2502}}
\newcommand{\pontusnine}{\textcolor{magenta}{pontus\_2489}}
\newcommand{\pontusfivezerosix}{\textcolor{magenta}{pontus\_2506}}
\newcommand{\rollingmixopsim}{\textcolor{magenta}{rolling\_mix\_10yrs\_opsim}}
\newcommand{\rollingopsim}{\textcolor{orange}{rolling\_10yrs\_opsim}}


\section{Strong Lensing}
\textit{Authors: Simon Huber\footnote{shuber@mpa-garching.mpg.de}, Sherry H.~Suyu\footnote{suyu@mpa-garching.mpg.de}, Tanja Petrushevska\footnote{tanja.petrushevska@ung.si}, Timo Anguita, Phil Marshall, Lynne Jones }

\

The Hubble constant $H_0$ is one of the key parameters to describe the
universe. Current observations of the CMB (cosmic microwave
background) assuming a flat $\Lambda$CDM cosmology and the standard
model of particle physics yield $H_0 = 67.36 \pm 0.54 \, {\rm km\,s^{-1}\,Mpc^{-1}}$
\citep{Planck:2018vks} which is in tension with $H_0 =
73.52 \pm
  1.62 \, {\rm km\,s^{-1}\,Mpc^{-1}}$ from the local distance ladder
\citep{Riess:2016jrr,Riess:2018byc}. In order to verify or refute this
$3.6 \sigma$ tension, further independent methods are needed. 

One such method is lensing time delay cosmography which can determine
$H_0$ in a single step. The basic idea is to measure the time delays
between multiple images of a strongly lensed variable source
\citep{Refsdal:1964}. This time delay, in combination with mass
profile reconstruction of the lens and line-of-sight mass structure,
yields directly a ``time-delay distance'' that is inversely
proportional to the Hubble constant ($t \propto D \propto
H_0^{-1}$). Applying this method to four lensed quasar systems, the
H0LiCOW collaboration \citep{Suyu:2016qxx} together with the
COSMOGRAIL collaboration
\citep[e.g.]{Eigenbrod:2005ie,2013Tewes,2017Courbin} measured $H_0 =
72.5^{+2.1}_{-2.3} \,{\rm km\,s^{-1}\,Mpc^{-1}}$ in flat
$\Lambda$CDM \citep{Birrer:2018vtm}, which is in agreement with the
local distance ladder and higher than CMB measurements.  Another
promising approach goes back to the initial idea of
\cite{Refsdal:1964} using lensed supernovae (LSNe) instead of quasars
for time-delay cosmography. Here we investigate the prospects of using
LSST for measuring time delays of both lensed supernovae and lensed
quasars.

\subsection{Supernovae Lensed by Galaxies}
\textit{Contributors: Simon Huber, Sherry H.~Suyu}

\

Even though the number of LSNe is significantly lower than the number of
lensed quasars, LSNe have some important advantages. First 
the sharp rise
and decline of SN light curves make time-delay measurements easier and possible
on shorter time scales in comparison to stochastically varying quasars. Second LSNe Ia are very promising
to break the model degeneracies \citep{Schneider:2013wga} in two
independent ways using dynamics \citep{Barnabe2011,2017:Yildirim} or
the standard candle nature of SNe Ia.  

So far only two systems with resolved
multiple images have been observed, namely SN ``Refsdal''
\citep{Kelly:2015xvu,Kelly:2015vjq} and iPTF16geu
\citep{Goobar:2016uuf}. But LSST will play a key role to detect many
more LSNe. At the moment we expect to find approximately $50$ resolved
LSNe Ia \citep{Oguri:2010} or $900$ in total \citep{Goldstein:2017bny}
over the 10 year survey. No other survey is capable of providing such
high numbers.

The goal of this section is to evaluate different
cadences for LSNe time-delay measurements. For this purpose we have investigated
20 different observing strategies: 15 from the call for whitepapers
(baseline2018a, kraken\_2026, colossus\_2665, pontus\_2002,
colossus\_2664, colossus\_2667, pontus\_2489, kraken\_2035,
mothra\_2045, pontus\_2502,
kraken\_2036, kraken\_2042, kraken\_2044, mothra\_2049, nexus\_2097)\footnote{\url{http://astro-lsst-01.astro.washington.edu:8080/}},
pontus\_2506 which is a cadence from Tiago Ribeiro doing the revisit
after 30 minutes in different filters, alt\_sched and
alt\_sched\_rolling from Daniel Rothchild and collaborators\footnote{\url{http://altsched.rothchild.me:8080/}}, and rolling\_10yrs\_opsim and rolling\_mix\_10yrs\_opsim from Peter Yoachim\footnote{https://github.com/yoachim/SLAIR\_runs}. 

For a better interpretation we subdivide the strategies into three categories: 

(1) \textcolor{orange} {baseline like in terms of inter-night gap and cumulative season length} (mean values), (2) \textcolor{blue}{shorter inter-night gap, shorter cumulative season length} and (3) \textcolor{magenta}{shorter inter-night gap, baseline like cumulative season length}. In addition cadences with a \textbf{large nominal WFD footprint} start with a capital letter.

The mean cumulative season length and mean inter-night gap for the categorization are shown in Figure \ref{fig:cadences categories}. The results are calculated from simulations of the 20 observing strategies by taking the mean of all fields under consideration. The mean cumulative season length is calculated by taking the mean of the summed up season length over all seasons. For the inter-night gap the revisits of a field within hours in the same filter are summarized into a single visit. We look at two different cases. The first case considers 719 LSST fields where all lie in the WFD survey. These fields are picked by taking all fields with center in $\mathrm{Dec} \in [-58,-2] \, \si{\deg}$ and  $\mathrm{RA} \in [0,120] \cup [330,360] \, \si{\deg}$, where all DDFs are excluded. By choice the Galactic Plane, the South Celestial Pole and the Northern Ecliptic are completely excluded and the 719 fields represent only the WFD survey and are used to classify the cadences. For comparison we consider in the second case all 5292 LSST fields covering the entire sky, where we only take into account those fields of the 5292 where observations are taken.


\begin{figure}
\centering
\includegraphics[scale=0.6]{figures/sl_cadences.pdf}
\caption{The mean inter-night gap (upper panel) and mean cumulative season length (lower panel) for 20 different observing strategies for two cases. The first case ("WFD" in solid black) considers 719 LSST fields, which all lie in the WFD survey. The shaded region encloses the 99th percentile of the WFD fields. The second case ("all" in dotted blue) considers all LSST fields (5292) where observations are taken. In the upper panel cadences with the black line below the black dot-dashed line are those with a significantly better inter-night gap than the baseline cadence (magenta and blue strategies). By looking at the lower panel these are subdivided into strategies with a cumulative season length similar to the baseline cadence (magenta) and a significantly worse cumulative season length (blue). Observing strategies in orange have a baseline like mean inter-night gap and mean cumulative season length.}
\label{fig:cadences categories}
\end{figure}

To simulate observations randomly, we have used 202
mock LSNe Ia from the OM 10 catalog \citep{Oguri:2010},
and produced the light curves for the mock SNe images with
the spherically symmetric SN Ia W7 model \citep{1984:Nomoto}
calculated with ARTIS (Applied Radiative Transfer In Supernovae)
\citep{Kromer:2009ce} in combination with magnifications maps from
GERLUMPH \citep{Vernardos:2015wta} to include the effect of
microlensing similar as in \citep{Goldstein:2017bny}. We then simulate
data points for the light curves, following the observation pattern from different cadences
and uncertainties according to the LSST science book
\citep{2009:LSSTscience}. To measure the time delay from the simulated
observation we use the free knot spline optimizer from PyCS (Python
Curve Shifting) \citep{2013:Tewesb,Bonvin:2015jia}. Details of this
work will be presented in Huber et al. (in preparation).


The structure of this subsection is organized as follows. In
\ref{sec:simulation of mock data} we describe how we simulate and
evaluate the mock data and in \ref{sec:results} we present our results
where we have quantified 20 cadences for LSNe Ia.

\subsubsection{Simulating and evaluating mock data}
\label{sec:simulation of mock data}
To simulate mock data for the different cadences we have picked 10
fields in the WFD (wide fast deep survey) which are listed in Table
\ref{tab: 10 wfd fields}. For a given cadence for each of these
fields, we store the following for each visit of the field: date
(mjd), filter(s) observed, and 5-$\sigma$ depth $m_5$. Such an
observing sequence of visits is illustrated for the ``baseline2018a''
cadence in figure \ref{fig:observation patter LSST 10 year survey},
where for one field in the WFD all observations within the 10 year
survey are shown. 
%
\begin{figure}
\centering
\includegraphics[scale=0.7]{figures/sl_field_number3_baseline2018a_Daniel.pdf}
\caption{This illustrates for the observing strategy ``baseline2018a'' the mjd and filter when observations are taken over the 10 year survey for field number 4 in table \ref{tab: 10 wfd fields} in the wide fast deep survey}
\label{fig:observation patter LSST 10 year survey}
\end{figure}
%
%\FloatBarrier 
To simulate observational data of a LSN system, we place randomly in
one of the 10 choosen fields from Table \ref{tab: 10 wfd fields} a
mock LSNe Ia from the OM 10 catalog, which is a mock catalog for
strong gravitational lenses \citep{Oguri:2010}. The catalog contains
about 400 LSNe Ia (The catalog is 10 times oversampled which means
that the total number of LSNe Ia is about 40) for LSST with an image
separation larger than $\SI{0.5}{\arcsec}$. For the lens model an SIE
(singular isothermal ellipsoid) \citep{Kormann:1994} is assumed and
therefore the time delay $\tau$, the convergence $\kappa$ and the
shear $\gamma$ is known for each of the multiple SNe images. By
assuming a W7 model and placing each of the SNe images randomly in the
corresponding magnification map one can calculate mock light
curves. Furthermore we place the mock system randomly in time, such
that the detection criterion applied in OM 10 is fulfilled. The
criterion is that the peak of the i-band magnitude of the fainter image
for a double or the 3rd brightest image for a quad, falls in the
observing season.
By combining this with the observing sequence, we get simulated
observations as illustrated in figure \ref{fig: simulated observation}
for a quad system. The error is calculated according to \cite[sec 3.5,
p. 67]{2009:LSSTscience}.
\begin{figure}[h!]
\centering
\includegraphics[scale=0.7]{figures/sl_Obsevation_number399_baseline2018a_filter_i_oversampling_00.pdf}
\caption[]{In this figure the i-band light curves of a mock quad LSNe Ia are shown. The observation sequence is for a random field in WFD survey for the cadence ``baseline2018a''.}
\label{fig: simulated observation}
\end{figure}
%
\begin{table}
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
field number & 1 & 2 & 3 & 4 & 5& 6 & 7 & 8 & 9 & 10  \\
\hline
RA& 0.0 & 32.1 & 65.8 & 50.9 &44.9& 125.6 & 155.0 & 207.7 & 304.3 & 327.5  \\
\hline
DEC& -7.4 & -44.2 & -7.2 & -30.0 & -50.9& -11.4 & -25.6 & -45.3 & -55.2 & -35.9  \\
\end{tabular}
\caption{The 10 fields of the wide fast deep survey, where the observation sequence for different cadences was considered.}
\label{tab: 10 wfd fields}
\end{table}
%
%\FloatBarrier

To evaluate the mock data and get a measured time delay we use the
free knot spline optimizer from PyCS (Python Curve Shifting)
\citep{2013:Tewesb,Bonvin:2015jia}. PyCS was initially developed to
measure time delays in strongly lensed quasars, and is not yet
optimised for LSNe Ia, such as fitting simultaneously multiple filters
and using SN template light curves.  Applying PyCS to individual
filter's light curves, we get a single independent time delay for each
filter.  We combine the 6 delays from the 6 LSST filters afterwards
into a single delay, but we expect more precise and accurate delays by
using multi-color fitting in the future. We also expect improvements
in delay measurements with the use of SNe Ia template instead of
splines.  

%so there are still
%some improvements for LSNe Ia, which will be implemented in the
%future. The first one is that to date multi-color time delay fitting
%is not possible, which means that we get a single independent time
%delay for each filter. We combine this 6 delays afterwards to a single
%delay, but we expect more precise and accurate delays by using
%multi-color fitting. A second improvement might be the use of SNe Ia
%templates instead of splines. This means that we do a conservative
%time delay estimate and with future improvements we might be able to
%measure time delays even better. 

To have sufficient statistics, we investigate for each cadence
strategy 202 mock LSNe Ia, where we pick 50 \% doubles and 50 \%
quads. For each of the mock systems we draw 100 random starting
configurations. A starting configuration corresponds to a random
position in the microlensing map and a random field from Table
\ref{tab: 10 wfd fields}, where it is placed randomly in one of the
observing seasons such that the detection requirement from OM 10 is
fulfilled. For each of these starting configurations we then draw 1000
different noise realizations of light curves, where we also shift the
time delays for each noise realizations randomly by $-3$ to
$\SI{3}{\day}$, to estimate uncertainties of delay measurements with
PyCS. For each realization we calculate the deviation from the true
time delay as
%
\begin{equation}
\tau_\mathrm{d} = \frac{t_\mathrm{measured} - t_\mathrm{true}}{t_\mathrm{true}}.
\label{eq: deviation from true time delay}
\end{equation}
For one strategy and double LSNe Ia, we have thus $1 {\rm (delay\ for\ the\
one\ pair\ of\ images)} \times 6 {\rm (filters)} \times 100 {\rm
(starting\ configurations)} \times 1000 {\rm (noise\ realisations)}$
time-delay deviations as in \eqref{eq: deviation from true time delay}.
%, where the 6 stands for the 6 LSST filters. 
For the 6 pairs of images for a quad system we have a sample of $6
\times 6 \times 100 \times 1000$. The resulting distribution of
time-delay deviation is investigated for each pair of images and each
filter separately. From the $100 \times 1000$ time-delay deviations we
define accuracy as the median $\tau_\mathrm{d,50}$ and precision as
$\delta = (\tau_\mathrm{d,84}-\tau_\mathrm{d,16})/2$, where
$\tau_\mathrm{d,84}$ is the 84th and $\tau_\mathrm{d,16}$ the 16th
percentile. Measuring $H_0$ with 1\% accuracy requires that the accuracy
in the delay deviation $\tau_\mathrm{d,50}$ is $<1\%$ (since $H_0 \propto
t_{\rm true}^{-1}$). Since the 6 time-delay deviations from the 6 filters are independent we combine them into a single time-delay deviation via the weighted mean. This means that in the end we have for one strategy and a mock LSNe Ia one 
\begin{equation}
\tau_\mathrm{d,50} \pm \delta
\label{eq: accuracy and precission}
\end{equation}
per pair of images.


\subsubsection{Results}
\label{sec:results}
In this section we summarize the results and quantify the 20
investigated cadences. Given that $H_0 \propto \frac{1}{t}$, where $t$
is the time delay between two images, we aim for accuracy
($\tau_\mathrm{d,50}$) smaller than 1 percent and precision ($\delta$)
smaller than 5 percent in equation \ref{eq: accuracy and precission}. The accuracy
requirement is needed for measuring $H_0$ with 1\% uncertainty, and
the precision requirement ensures that the delay uncertainty does not
dominate the overall uncertainty on $H_0$ given typical mass modeling
uncertainties of $\sim 5\%$ \citep[e.g.,][]{Suyu2018}.  A quad system is counted as successful if one of the 6 delays fulfills this requirement. 
We have investigated two different cases, first using LSST data only to measure time delays and second, using LSST as a discovering machine in combination with follow-up observations to measure delays. 
For just using LSST data we have investigated 202 mock LSNe Ia and for using LSST in combination with follow-up 100 mock systems have been investigated, where for each case 50 \% are doubles and 50 \% are quads. 
We assume follow-up observation would start 2 days 
after the third LSST data point in any filter exceeds the 5-$\sigma$ 
depth, where the follow-up is done in 3 filters (g,r,i) every second night.
The fraction of systems for which the time-delay measurement fulfills the above defined requirement is summarized in columns 3 and 5 of Table \ref{tab: quantify different observing strategies}. These numbers have to be combined with the total number of LSNe Ia we expect to detect for different strategies. We approximate the total number of LSNe Ia as

\begin{align}
\label{eq: total number of LSNe Ia from modified OM 10}
N_\mathrm{LSNe Ia, cad} = N_\mathrm{LSNe Ia, OM 10} \frac{\Omega_\mathrm{cad}}{\Omega_\mathrm{OM 10}} \frac{\bar{t}_\mathrm{eff,cad}}{t_\mathrm{eff, OM 10}}
\end{align}
%
where $N_\mathrm{LSNe Ia, OM10} = 45.7$, $\Omega_\mathrm{OM10} = \SI{20000}{\square\deg}$ and $t_\mathrm{eff, OM10}=\SI{2.5}{\year}$ from \cite{Oguri:2010}. $\bar{t}_\mathrm{eff,cad}$ is the mean effective/cumulative seasonal length for a given cadence strategy, where we have averaged over all 719 WFD fields. $\Omega_\mathrm{cad}$ is the survey area for a given observing strategy. Instead of taking the nominal values (see column 2 Table \ref{tab:LSST Survey Area for different observing strategies}) we calculate the area from fields represented by our study. These are fields with a mean cumulative season length and inter-night gap similar or even better than the 719 WFD fields, meaning: Cumulative season length ($t_\mathrm{eff}$) longer than the lower 99th percentile and inter-night gap ($t_\mathrm{gap}$) shorter than the upper 99th percentile. Further we also take into account the 5-$\sigma$ depth ($m_5$)\footnote{Here we consider only the main relevant bands g,r,i and z.}. Here we consider all fields with ($m_5+0.2 \mathrm{mag}$) greater than the lower 99th percentile of the 719 WFD fields. The relaxed 5-$\sigma$ depth is necessary in order to represent the wider areas as suggested by the nominal values. The area can then be calculated from the number of systems fulfilling the above defined criteria times the field of view of $\SI{9.6}{\square\deg}$ taking into account the overlap factor of the fields:
%
\begin{align}
\Omega_\mathrm{cad} = f_\mathrm{overlap} \cdot N_\mathrm{cad,criteria} \cdot \SI{9.6}{\square\deg},
\end{align}
where
$f_\mathrm{overlap}=\frac{5292 \cdot \SI{9.6}{\square\deg}}{4 \pi \cdot (\SI{180}{\deg}/\pi)^2} \approx 0.812.$
The calculated area is shown in column 3 of Table \ref{tab:LSST Survey Area for different observing strategies}. They still represent the wider areas as suggested by the nominal values, but are more representative in terms of delay measurements than the nominal areas because they contain observational constraints on cumulative season length, inter-night gap and 5-$\sigma$ depth\footnote{The 0.2 mag difference in the 5-$\sigma$ depth would reduce for the LSST + follow-up case the number of detected systems by a few percents, whereas, it does not matter for the case of using LSST only.}.

\begin{table}
\centering

\begin{tabular}{c|c|c|c}                                                                                               
& $\Omega_\mathrm{nom}$ & $\Omega_\mathrm{cad}$ &  $(\Omega_\mathrm{cad}-\Omega_\mathrm{nom})/\Omega_\mathrm{nom} [\%]$ \\
\hline
\altsched          &  18000 &  17703 &  -1.6 \\
\altschedrolling   &  18000 &  20463 &  13.7 \\
\baseline          &  18000 &  17306 &  -3.9 \\
\colossusfour      &  18000 &  18202 &   1.1 \\
\colossusfive      &  18000 &  18475 &   2.6 \\
\colossusseven     &  18000 &  17797 &  -1.1 \\
\krakentwosix      &  18000 &  17119 &  -4.9 \\
\krakenfive        &  18000 &  17680 &  -1.8 \\
\krakenthreesix    &  18000 &  17719 &  -1.6 \\
\krakentwo         &  18000 &  17828 &  -1.0 \\
\krakenfour        &  24700 &  24010 &  -2.8 \\
\mothrafive        &  18000 &  16417 &  -8.8 \\
\mothranine        &  24700 &  21874 & -11.4 \\
\nexusseven        &  24700 &  20471 & -17.1 \\
\pontuszerozerotwo &  24700 &  22926 &  -7.2 \\
\pontusnine        &  18000 &  17758 &  -1.3 \\
\pontusfivezerotwo &  18000 &  17602 &  -2.2 \\
\pontusfivezerosix &  18000 &  18132 &   0.7 \\
\rollingopsim      &  18000 &  18148 &   0.8 \\
\rollingmixopsim   &  18000 &  18132 &   0.7 \\
\end{tabular} 
 \caption{Survey areas for different observing strategies. The second column contains the nominal values and the third column shows the area used for Equation \ref{eq: total number of LSNe Ia from modified OM 10} taking into account observational constraints (see text). The forth column shows the deviation from the nominal values in percent.}
 \label{tab:LSST Survey Area for different observing strategies}
\end{table}




The results in terms of total number of LSNe Ia for the investigated cadences are shown in column 6 of Table \ref{tab: quantify different observing strategies}, where we see that for most of the rolling cadences (blue strategies) fewer LSNe Ia will be detected, because of the shorter cumulative season lengths $\bar{t}_\mathrm{eff,cad}$. The total number depends on the selection criteria assumed in \cite{Oguri:2010}. If we relax on the criteria like the image separation these numbers will be higher, but the order of the strategies will be unchanged.

Columns 2 and 4 in Table \ref{tab: quantify different observing strategies} contain the total number of LSNe Ia where the delay can be measured with accuracy $<$ 1 \% and precision $<$ 5 \% over the 10 year survey. For the case of using only LSST data, we see that even for the best strategies we will just have
a few systems where time-delay measurements are possible. Follow-up observations are therefore necessary to
increase the number of LSNe Ia with delays as shown in column 2 of Table \ref{tab: quantify different observing strategies}. These results are also more qualitatively summarized in Table \ref{tab: favoured strategies}

To summarize, for our science case of measuring time delays from as many lensed SNe as possible, it would be more effective to use LSST as a discovering machine with additional follow-up, instead of relying on LSST completely for the delay measurements. From our investigations we find that long cumulative seasonal lengths $\bar{t}_\mathrm{eff,cad}$ and a more frequent sampling are important. To go more into detail, we request for 10 seasons with a season length of 170 days or longer. Rolling cadences are clearly disfavored, because their shortened cumulative season lengths lead to overall a more negative impact on the number of LSNe Ia with delays, compared to the gain from the increased sampling frequency\footnote{rolling\_mix\_10\_yrs\_opsim has a cumulative season length close to baseline2018a and does the revisit in different filters. It is therefore the only rolling cadence performing better than the baseline cadence.}. To improve the sampling, single visits per night would be helpful. Since this will make the science case of fast-moving transients impossible we suggest doing one revisit within a night but in a different band than the first visit. Further improvements are the replacement of $2 \times \SI{15}{\s}$ exposure by $1 \times \SI{30}{\s}$ for an increased efficiency and redistributing some of the visits in y band to g, r, i and z.
%of their shorter cumulative season lengths $\bar{t}_\mathrm{eff,cad}$ although they improve the sampling. Therefore we reject to improve on one of the 3 parameters our science case is mostly sensitive to, by worsen at the same time one of the others significantly. 
%

Further \cite{Goldstein:2018bue} performed detailed simulations of the gLSN population using a completely independent technique and pipeline and reached similar conclusions to the ones presented here: rolling cadences are strongly disfavored, and wide-area, long-season surveys with well sampled light curves are optimal.


\begin{table}
\centering
\begin{tabular}{c|cc|cc|c}
\multicolumn{1}{c}{}& \multicolumn{2}{c}{\textbf{LSST + follow-up}}  & \multicolumn{2}{c}{\textbf{LSST only}} & \multicolumn{1}{c}{ } \\

& total  & total   & total  & total  & total \\
& number  & fraction  & number  & fraction  & number \\
&  with & with & with& with & of\\
& delays & delays& delays& delays&LSNe Ia\\
\hline
\krakenfour        &  27.7 &  27.2 &  5.9 &   5.8 &  102.0 \\
\colossusseven     &  27.5 &  32.7 &  7.1 &   8.4 &   84.0 \\
\altsched          &  21.8 &  35.3 &  7.9 &  12.8 &   61.7 \\
\pontusfivezerosix &  20.1 &  27.8 &  6.1 &   8.4 &   72.2 \\
\krakentwo         &  19.7 &  25.2 &  4.5 &   5.8 &   78.0 \\
\pontusnine        &  19.3 &  23.8 &  6.0 &   7.4 &   81.1 \\
\rollingmixopsim   &  18.9 &  23.8 &  7.6 &   9.5 &   79.4 \\
\krakentwosix      &  18.7 &  25.8 &  3.5 &   4.8 &   72.4 \\
\pontuszerozerotwo &  18.1 &  21.0 &  1.2 &   1.4 &   86.1 \\
\colossusfive      &  17.2 &  22.4 &  2.9 &   3.7 &   76.8 \\
\baseline          &  16.5 &  22.4 &  2.7 &   3.7 &   73.4 \\
\colossusfour      &  15.7 &  21.0 &  3.1 &   4.1 &   74.6 \\
\krakenfive        &  15.5 &  21.0 &  2.0 &   2.7 &   73.4 \\
\altschedrolling   &  13.7 &  35.9 &  6.3 &  16.5 &   38.0 \\
\pontusfivezerotwo &  13.5 &  17.7 &  1.0 &   1.4 &   76.3 \\
\nexusseven        &  12.5 &  23.8 &  2.5 &   4.8 &   52.3 \\
\mothranine        &  12.2 &  23.8 &  2.4 &   4.7 &   51.0 \\
\rollingopsim      &  11.9 &  14.9 &  5.4 &   6.8 &   79.8 \\
\krakenthreesix    &  11.4 &  25.2 &  2.1 &   4.7 &   45.2 \\
\mothrafive        &  10.4 &  27.9 &  2.3 &   6.1 &   37.3 \\


\end{tabular}
\caption{This table quantifies 20 different cadence strategies for measuring time delays in LSNe Ia. The second and third column consider LSST as a discovery machine in combination with follow-up observations in 3 filters (g,r,i) every second night. We assume follow-up observation would start 2 days after the third LSST data point in any filter exceeding the 5-$\sigma$ depth. In the fourth and fifth column only LSST data is used to measure time delays. The sixth column shows the total amount of LSNe Ia (69 \% doubles and 31 \% quads) calculated via \eqref{eq: total number of LSNe Ia from modified OM 10}. The columns "total fraction with delays" contain the fraction of the  investigated mock LSNe Ia where the time delay could be measured with accuracy better than 1 percent and precision better than 5 percent. We have investigated 100 mock systems for LSST + follow-up and 202 mock systems for the case where only LSST data is used. The columns "total number with delays" combine the columns "total fraction with delays" with the column "total number of LSNe Ia". Since for LSST only, the total numbers with delays are very low, we advocate using LSST as a discovering machine with observational follow up. Therefore the second column is the relevant one to quantify different cadences. For a more qualitative conclusion see Table \ref{tab: favoured strategies}.}
\label{tab: quantify different observing strategies}
\end{table}
%
\begin{table}
\centering
\begin{tabular}{c|c|c|c}
& good with obs.   &  favored with . \\
& follow-up in addition to LSST  & LSST data only \\
\hline
\krakenfour        &  x &  x \\
\hline
\colossusseven     &  x &  x \\
\hline
\altsched          &  x &  x \\
\hline
\pontusfivezerosix &  x &  x \\
\hline
\krakentwo         &  x &   \\
\hline
\pontusnine        &  x &  x \\
\hline
\rollingmixopsim   &  x &  x \\
\hline
\krakentwosix      &  x &  \\
\hline
\pontuszerozerotwo &  x &  \\
\hline
\colossusfive      &  x &   \\
\hline
\baseline          &  x &  \\
\hline
\colossusfour      &  &  \\
\hline
\krakenfive        &  &   \\
\hline
\altschedrolling   &  &  x \\
\hline
\pontusfivezerotwo &  &   \\
\hline
\nexusseven        &   &   \\
\hline
\mothranine        &  &   \\
\hline
\rollingopsim      & &  x \\
\hline
\krakenthreesix    &  &   \\
\hline
\mothrafive        &  &  \\

\end{tabular}
\caption{This table ranks 20 different cadences for the scenarios combining LSST with follow-up and using LSST data only. Cadences marked with a "x" are good for the given scenario, where unmarked ones are disfavored.}
\label{tab: favoured strategies}
\end{table}
%
\FloatBarrier
\subsection{Supernovae Lensed by Galaxy Clusters}
\textit{Contributor: Tanja Petrushevska}

\

Here, we focus on prospects of observing supernovae which are
lensed by known galaxy clusters. High-z galaxies that appear as
multiple images in the cluster field can host supernova
explosions. Strongly lensed supernovae by galaxy clusters not only
can be used as tools to examine both global cosmology, but also
the local environment of the cluster lenses. Cluster lensing time
scales are typically much longer and the microlensing effects are
almost negligible, which makes their measurement potentially more
feasible, especially if the lens potential is well studied and the
predicted time delays have small uncertainties. We calculate the
expected number of supernovae Ia in the multiply lensed background
galaxies by using the Hubble Frontier Fields cluster and Abell
1689. These clusters have been extensively studied, and given the
good quality data, well constrained magnification maps and time
delays can be obtained from the lensing models. We only considered
those that have a spectroscopic redshift. To obtain better image
depth, we combine the images that are taken closer than 5 days in
time. We note that these are a lower limits, since we have only
considered few clusters and the galaxies with spectroscopic
redshift. For this science case, the most important bands are i, z
and y. Since most of the light of nearby SNe is in the optical
bands, these filters are optimal for finding high-z SNe, as their
light is redshifted to the longer wavelengths. When we consider
the different observing strategies, we find that the rolling
cadences (mothra\_2045 and pontus\_2502) are disfavored given that
they do not return to the clusters as the other observing
strategies. The strategy pontus\_2489 provides slightly better
prospects compared to the others because it provides the most
number of visits to the same cluster fields.

\begin{figure}
\centering
\includegraphics[scale=0.65]{figures/sl_galaxy_lensing.pdf}\caption{The expected total number of strongly lensed SNe Ia arising from the multiply imaged galaxies in the Hubble Frontier Fields and Abell 1689 in function of the observing strategy. \todo{Tanja}{include new cadences and do same estimates for CC SNe*}}
\end{figure}


\subsection{Lensed Quasars\footnote{Summarized and updated version of  \citep[Cosmology chapter of][]{LSSTScienceCollaboration2017}}}
\textit{Contributors: Phil Marshall, Timo Anguita, Lynne Jones}


The goal of this section is to evaluate the precision we can achieve in measuring time delays in strongly lensed AGN, and as such, the precision on the measurement of the Hubble constant from all systems with measured time delays.

Anticipating that the time delay accuracy would depend on night-to-night
cadence, season length, and campaign length, we carried out a large
scale simulation and measurement program that coarsely sampled these
schedule properties. In \cite{Liao2015}, we simulated 5 different
light curve datasets, each containing 1000 lenses, and presented them to
the strong lensing community in a ``Time Delay Challenge.'' These 5
challenge ``rungs'' differed by their schedule properties. Focusing on the best challenge
submissions made by the community, we derived a simple power law model
for the variation of each of the time delay accuracy, time delay
precision, and useable sample fraction, with the schedule properties
cadence, season length and campaign length. They are
given by the following equations:

\begin{align}
|A|_{\rm model} &\approx 0.06\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{0.0}
\left(\frac{\rm sea}  {\rm 4 months}\right)^{-1.0}
\left(\frac{\rm camp}{\rm 5 years} \right)^{-1.1}  \notag \\
P_{\rm model} &\approx 4.0\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{ 0.7}
\left(\frac{\rm sea}  {\rm 4 months}\right)^{-0.3}
\left(\frac{\rm camp}{\rm 5 years} \right)^{-0.6}  \notag \\
f_{\rm model} &\approx 30\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{-0.4}
\left(\frac{\rm sea}  {\rm 4 months}\right)^{ 0.8}
\left(\frac{\rm camp}{\rm 5 years} \right)^{-0.2} \notag 
\end{align}

All three of these diagnostic metrics would, in an ideal world, be
optimized: this could be achieved by decreasing the night-to-night
cadence (to better sample the light curves), extending the observing
season length (to maximize the chances of capturing a strong variation
and its echo), and extending the campaign length (to increase the number
of effective time delay measurements).

The quantity of greatest scientific interest is the {\it accuracy in
	cosmological parameters}: this could be computed as follows. Setting a
required accuracy threshold  defines the available number of lenses,
which in turns gives us the mean precision per lens there. Combining the
whole sample, we would get the error on the weighted mean time delay, as
used by \cite{Coe2009}. This uncertainty, which scales as one
over the square root of the number of available lenses,  can be roughly
equated to the statistical uncertainty on the Hubble constant.

Our Opsim analysis consists in selecting only the sky survey area that allows time delay measurements with accuracies of $<0.2\%$ \citep{Hojjati2014}. This high accuracy area can be used
to define a ``Gold Sample'' of lenses, whose mean precision per lens we
can compute. The TDC2 useable fraction averaged over this area gives us
the approximate size of this sample: we simply re-scale the 400 lenses
predicted by \cite{Liao2015} by this fraction over the 30\% found
in TDC2. While these numbers are approximate, the ratios between
different observing and analysis strategies provide a useful indication of relative merit.

As described above, we follow \cite{Coe2009} and compute a
very simple time delay distance Figure of Merit ``\texttt{DPrecision}''
as follows. We first combine the fractional time delay precision in
quadrature with an assumed 4\% ``modeling uncertainty,'' and then divide
this by the square root of the number of Gold Sample lenses. This
estimated ensemble distance precision can be straightforwardly related
to cosmological parameter precision, as cite{Coe2009} show
(it's very roughly the precision on the Hubble constant).

Our calculations are performed using the full 10 years of LSST operations with both the single i-band observations and all bands merged together. Naturally, significantly better results are obtained making no distinctions between photometric bands, however, it is important to note that there is a rather large caveat: Even when AGN variability can show almost negligible difference between bands close in wavelength, the difference can be important between the bluest and reddest LSST bands. As such, the i-band results can be interpreted as a very conservative upper limit in the precision attainable and the "all-bands" result as a very optimistic lower limit.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/sl_QSO_Nlens.png}    
		\caption{Number of ``golden lenses'' in sky areas with accuracies $<$0.2\%}   
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{sl_QSO_Precision.png}    
		\caption{Mean precision per lens (percent) in  sky areas with accuracies $<$0.2\%}  
\end{figure}
\begin{figure}
\centering
		\includegraphics[width=\linewidth]{sl_QSO_Dprec.png}    
		\caption{Time delay distance precision figure of merit (percent) in  sky areas with accuracies $<$0.2\%} 
\end{figure}

