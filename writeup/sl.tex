% Need these new commands to compile:
%\newcommand{\todo}[2]{\textcolor{red}{\textbf{TODO (#1): #2}}}
%\newcommand{\comment}[1]{\textcolor{blue}{\textbf{#1}}}



\section{Strong Lensing}
\textit{Authors: Simon Huber\footnote{shuber@mpa-garching.mpg.de}, Sherry H.~Suyu\footnote{suyu@mpa-garching.mpg.de}, Tanja Petrushevska\footnote{tanja.petrushevska@ung.si} }

\

The Hubble constant $H_0$ is one of the key parameters to describe the
universe. Current observations of the CMB (cosmic microwave
background) assuming a flat $\Lambda$CDM cosmology and the standard
model of particle physics yield $H_0 = 67.36 \pm 0.54 \, {\rm km\,s^{-1}\,Mpc^{-1}}$
\citep{Planck:2018vks} which is in tension with $H_0 =
73.52 \pm
  1.62 \, {\rm km\,s^{-1}\,Mpc^{-1}}$ from the local distance ladder
\citep{Riess:2016jrr,Riess:2018byc}. In order to verify or refute this
$3.6 \sigma$ tension, further independent methods are needed. 

One such method is lensing time delay cosmography which can determine
$H_0$ in a single step. The basic idea is to measure the time delays
between multiple images of a strongly lensed variable source
\citep{Refsdal:1964}. This time delay, in combination with mass
profile reconstruction of the lens and line-of-sight mass structure,
yields directly a ``time-delay distance'' that is inversely
proportional to the Hubble constant ($t \propto D \propto
H_0^{-1}$). Applying this method to four lensed quasar systems, the
H0LiCOW collaboration \citep{Suyu:2016qxx} together with the
COSMOGRAIL collaboration
\citep[e.g.]{Eigenbrod:2005ie,2013Tewes,2017Courbin} measured $H_0 =
72.5^{+2.1}_{-2.3} \,{\rm km\,s^{-1}\,Mpc^{-1}}$ in flat
$\Lambda$CDM \citep{Birrer:2018vtm}, which is in agreement with the
local distance ladder and higher than CMB measurements.  Another
promising approach goes back to the initial idea of
\cite{Refsdal:1964} using lensed supernovae (LSNe) instead of quasars
for time-delay cosmography. Here we investigate the prospects of using
LSST for measuring time delays of both lensed supernovae and lensed
quasars.

\subsection{Supernovae Lensed by Galaxies}
\textit{Contributors: Simon Huber, Sherry H.~Suyu}

\

Even though the number of LSNe is significantly lower than the number of
lensed quasars, LSNe have some important advantages. First 
the sharp rise
and decline of SN light curves make time-delay measurements easier and possible
on shorter time scales in comparison to stochastically varying quasars. Second LSNe Ia are very promising
to break the model degeneracies \citep{Schneider:2013wga} in two
independent ways using dynamics \citep{Barnabe2011,2017:Yildirim} or
the standard candle nature of SNe Ia.  

So far only two systems with resolved
multiple images have been observed, namely SN ``Refsdal''
\citep{Kelly:2015xvu,Kelly:2015vjq} and iPTF16geu
\citep{Goobar:2016uuf}. But LSST will play a key role to detect many
more LSNe. At the moment we expect to find approximately $50$ resolved
LSNe Ia \citep{Oguri:2010} or $900$ in total \citep{Goldstein:2017bny}
over the 10 year survey. No other survey is capable of providing such
high numbers.

The goal of this section is to evaluate different
cadences for LSNe time-delay measurements. For this purpose we have investigated
18 different observing strategies: 15 from the call for whitepapers
(baseline2018a, kraken\_2026, colossus\_2665, pontus\_2002,
colossus\_2664, colossus\_2667, pontus\_2489, kraken\_2035,
mothra\_2045, pontus\_2502,
kraken\_2036, kraken\_2042, kraken\_2044, mothra\_2049, nexus\_2097)\footnote{\url{http://astro-lsst-01.astro.washington.edu:8080/}},
pontus\_2506 which is a cadence from Tiago Ribeiro doing the revisit
after 30 minutes in different filters, and alt\_sched and
alt\_sched\_rolling from Daniel Rothchild and collaborators
\footnote{\url{http://altsched.rothchild.me:8080/}}. 
To simulate observations randomly, we have used 202
mock LSNe Ia from the OM 10 catalog \citep{Oguri:2010},
and produced the light curves for the mock SNe images with
the spherically symmetric SN Ia W7 model \citep{1984:Nomoto}
calculated with ARTIS (Applied Radiative Transfer In Supernovae)
\citep{Kromer:2009ce} in combination with magnifications maps from
GERLUMPH \citep{Vernardos:2015wta} to include the effect of
microlensing similar as in \citep{Goldstein:2017bny}. We then simulate
data points for the light curves, following the observation pattern from different cadences
and uncertainties according to the LSST science book
\citep{2009:LSSTscience}. To measure the time delay from the simulated
observation we use the free knot spline optimizer from PyCS (Python
Curve Shifting) \citep{2013:Tewesb,Bonvin:2015jia}. Details of this
work will be presented in Huber et al. (in preparation).


The structure of this subsection is organized as follows. In
\ref{sec:simulation of mock data} we describe how we simulate and
evaluate the mock data and in \ref{sec:results} we present our results
where we have quantified 18 cadences for LSNe Ia.

\subsubsection{Simulating and evaluating mock data}
\label{sec:simulation of mock data}
To simulate mock data for the different cadences we have picked 10
fields in the WFD (wide fast deep survey) which are listed in Table
\ref{tab: 10 wfd fields}. For a given cadence for each of these
fields, we store the following for each visit of the field: date
(mjd), filter(s) observed, and 5-$\sigma$ depth $m_5$. Such an
observing sequence of visits is illustrated for the ``baseline2018a''
cadence in figure \ref{fig:observation patter LSST 10 year survey},
where for one field in the WFD all observations within the 10 year
survey are shown. 
%
\begin{figure}
\centering
\includegraphics[scale=0.7]{figures/sl_field_number3_baseline2018a_Daniel.pdf}
\caption{This illustrates for the observing strategy ``baseline2018a'' the mjd and filter when observations are taken over the 10 year survey for field number 4 in table \ref{tab: 10 wfd fields} in the wide fast deep survey}
\label{fig:observation patter LSST 10 year survey}
\end{figure}
%
%\FloatBarrier 
To simulate observational data of a LSN system, we place randomly in
one of the 10 choosen fields from Table \ref{tab: 10 wfd fields} a
mock LSNe Ia from the OM 10 catalog, which is a mock catalog for
strong gravitational lenses \citep{Oguri:2010}. The catalog contains
about 400 LSNe Ia (The catalog is 10 times oversampled which means
that the total number of LSNe Ia is about 40) for LSST with an image
separation larger than $\SI{0.5}{\arcsec}$. For the lens model an SIE
(singular isothermal ellipsoid) \citep{Kormann:1994} is assumed and
therefore the time delay $\tau$, the convergence $\kappa$ and the
shear $\gamma$ is known for each of the multiple SNe images. By
assuming a W7 model and placing each of the SNe images randomly in the
corresponding magnification map one can calculate mock light
curves. Furthermore we place the mock system randomly in time, such
that the detection criterion applied in OM 10 is fulfilled. The
criterion is that the peak of the i-band magnitude of the fainter image
for a double or the 3rd brightest image for a quad, falls in the
observing season.
By combining this with the observing sequence, we get simulated
observations as illustrated in figure \ref{fig: simulated observation}
for a quad system. The error is calculated according to \cite[sec 3.5,
p. 67]{2009:LSSTscience}.
\begin{figure}[h!]
\centering
\includegraphics[scale=0.7]{figures/sl_Obsevation_number399_baseline2018a_filter_i_oversampling_00.pdf}
\caption[]{In this figure the i-band light curves of a mock quad LSNe Ia are shown. The observation sequence is for a random field in WFD survey for the cadence ``baseline2018a''.}
\label{fig: simulated observation}
\end{figure}
%
\begin{table}
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
field number & 1 & 2 & 3 & 4 & 5& 6 & 7 & 8 & 9 & 10  \\
\hline
RA& 0.0 & 32.1 & 65.8 & 50.9 &44.9& 125.6 & 155.0 & 207.7 & 304.3 & 327.5  \\
\hline
DEC& -7.4 & -44.2 & -7.2 & -30.0 & -50.9& -11.4 & -25.6 & -45.3 & -55.2 & -35.9  \\
\end{tabular}
\caption{The 10 fields of the wide fast deep survey, where the observation sequence for different cadences was considered.}
\label{tab: 10 wfd fields}
\end{table}
%
%\FloatBarrier

To evaluate the mock data and get a measured time delay we use the
free knot spline optimizer from PyCS (Python Curve Shifting)
\citep{2013:Tewesb,Bonvin:2015jia}. PyCS was initially developed to
measure time delays in strongly lensed quasars, and is not yet
optimised for LSNe Ia, such as fitting simultaneously multiple filters
and using SN template light curves.  Applying PyCS to individual
filter's light curves, we get a single independent time delay for each
filter.  We combine the 6 delays from the 6 LSST filters afterwards
into a single delay, but we expect more precise and accurate delays by
using multi-color fitting in the future. We also expect improvements
in delay measurements with the use of SNe Ia template instead of
splines.  

%so there are still
%some improvements for LSNe Ia, which will be implemented in the
%future. The first one is that to date multi-color time delay fitting
%is not possible, which means that we get a single independent time
%delay for each filter. We combine this 6 delays afterwards to a single
%delay, but we expect more precise and accurate delays by using
%multi-color fitting. A second improvement might be the use of SNe Ia
%templates instead of splines. This means that we do a conservative
%time delay estimate and with future improvements we might be able to
%measure time delays even better. 

To have sufficient statistics, we investigate for each cadence
strategy 202 mock LSNe Ia, where we pick 50 \% doubles and 50 \%
quads. For each of the mock systems we draw 100 random starting
configurations. A starting configuration corresponds to a random
position in the microlensing map and a random field from Table
\ref{tab: 10 wfd fields}, where it is placed randomly in one of the
observing seasons such that the detection requirement from OM 10 is
fulfilled. For each of these starting configurations we then draw 1000
different noise realizations of light curves, where we also shift the
time delays for each noise realizations randomly by $-3$ to
$\SI{3}{\day}$, to estimate uncertainties of delay measurements with
PyCS. For each realization we calculate the deviation from the true
time delay as
%
\begin{equation}
\tau_\mathrm{d} = \frac{t_\mathrm{measured} - t_\mathrm{true}}{t_\mathrm{true}}.
\label{eq: deviation from true time delay}
\end{equation}
For one strategy and double LSNe Ia, we have thus $1 {\rm (delay\ for\ the\
one\ pair\ of\ images)} \times 6 {\rm (filters)} \times 100 {\rm
(starting\ configurations)} \times 1000 {\rm (noise\ realisations)}$
time-delay deviations as in \eqref{eq: deviation from true time delay}.
%, where the 6 stands for the 6 LSST filters. 
For the 6 pairs of images for a quad system we have a sample of $6
\times 6 \times 100 \times 1000$. The resulting distribution of
time-delay deviation is investigated for each pair of images and each
filter separately. From the $100 \times 1000$ time-delay deviations we
define accuracy as the median $\tau_\mathrm{d,50}$ and precision as
$\delta = (\tau_\mathrm{d,84}-\tau_\mathrm{d,16})/2$, where
$\tau_\mathrm{d,84}$ is the 84th and $\tau_\mathrm{d,16}$ the 16th
percentile. Measuring $H_0$ with 1\% accuracy requires that the accuracy
in the delay deviation $\tau_\mathrm{d,50}$ is $<1\%$ (since $H_0 \propto
t_{\rm true}^{-1}$). Since the 6 time-delay deviations from the 6 filters are independent we combine them into a single time-delay deviation via the weighted mean. This means that in the end we have for one strategy and a mock LSNe Ia one 
\begin{equation}
\tau_\mathrm{d,50} \pm \delta
\label{eq: accuracy and precission}
\end{equation}
per pair of images.


\subsubsection{Results}
\label{sec:results}
In this section we summarize the results and quantify the 18
investigated cadences. Given that $H_0 \propto \frac{1}{t}$, where $t$
is the time delay between two images, we aim for accuracy
($\tau_\mathrm{d,50}$) smaller than 1 percent and precision ($\delta$)
smaller than 5 percent in equation \ref{eq: accuracy and precission}. The accuracy
requirement is needed for measuring $H_0$ with 1\% uncertainty, and
the precision requirement ensures that the delay uncertainty does not
dominate the overall uncertainty on $H_0$ given typical mass modeling
uncertainties of $\sim 5\%$ \citep[e.g.,][]{Suyu2018}.  A quad system is counted as successful if one of the 6 delays fulfills this requirement. We have investigated two different cases, first using LSST data only to measure time delays and second, using LSST as a discovering machine in combination with follow-up observations to measure delays. For just using LSST data we have investigated 202 mock LSNe Ia and for using LSST in combination with follow-up 100 mock systems have been investigated, where for each case 50 \% are doubles and 50 \% are quads. We assume follow-up observation would start 2 days after the third LSST data point in any filter exceeds the 5-$\sigma$ depth, where the follow-up is done in 3 filters (g,r,i) every second night.
 
The fraction of systems for which the time-delay measurement fulfills the above defined requirement is summarized in columns 3 and 5 of Table \ref{tab: quantify different observing strategies}. These numbers have to be combined with the total number of LSNe Ia we expect to detect for different strategies. We approximate the total number of LSNe Ia as

\begin{align}
\label{eq: total number of LSNe Ia from modified OM 10}
N_\mathrm{LSNe Ia, cad} = N_\mathrm{LSNe Ia, OM 10} \frac{\Omega_\mathrm{cad}}{\Omega_\mathrm{OM 10}} \frac{\bar{t}_\mathrm{eff,cad}}{t_\mathrm{eff, OM 10}}
\end{align}
%
where $N_\mathrm{LSNe Ia, OM 10} = 45.7$, $\Omega_\mathrm{OM 10} = \SI{20000}{\square\deg}$ and $t_\mathrm{eff, OM 10}=\SI{2.5}{\year}$ from \cite{Oguri:2010}. $\Omega_\mathrm{cad}$ is the survey area for a given cadence. We assume $\Omega_\mathrm{cad}=\SI{24700}{\square\deg}$ for pontus\_2002, kraken\_2044, mothra\_2049 and nexus\_2097, and for the other strategies $\Omega_\mathrm{cad}=\SI{18000}{\square\deg}$. $\bar{t}_\mathrm{eff,cad}$ is the cumulative seasonal length for a given cadence, where we have averaged over all LSST fields where observations are taken. The results for the investigated cadences are shown in column 6 of Table \ref{tab: quantify different observing strategies}, where we see that for a rolling cadence fewer LSNe Ia will be detected, because of the shorter cumulative season lengths $\bar{t}_\mathrm{eff,cad}$. The total number depends on the selection criteria assumed in \cite{Oguri:2010}. If we relax on the criteria like the image separation these numbers will be higher, but the order will be unchanged.


Columns 2 and 4 in Table \ref{tab: quantify different observing strategies} contain the total number of LSNe Ia where the delay can be measured with accuracy $<$ 1 \% and precision $<$ 5 \% over the 10 year survey. For the case of using only LSST data, we see that even for the best strategies we will just have
a few systems where time-delay measurements are possible. Follow-up observations are therefore necessary to
increase the number of LSNe Ia with delays as visible in column 2 of Table \ref{tab: quantify different observing strategies}. These results are also more qualitatively summarized in Table \ref{tab: favoured strategies}

To summarize, for our science case of measuring time delays from as many lensed SNe as possible, it would be more effective to use LSST as a discovering machine with additional follow-up, instead of relying on LSST completely for the delay measurements. Therefore we are fine with the current baseline cadence. To improve on this cadence longer cumulative seasonal lengths $\bar{t}_\mathrm{eff,cad}$, bigger survey areas $\Omega_\mathrm{cad}$ and a more frequent sampling are helpful. The most important result from our investigation is that rolling cadences are clearly disfavored, because their shortened cumulative season lengths $\bar{t}_\mathrm{eff,cad}$ lead to overall a more negative impact on the number of LSNe Ia with delays, compared to the gain from the increased sampling frequency.
%of their shorter cumulative season lengths $\bar{t}_\mathrm{eff,cad}$ although they improve the sampling. Therefore we reject to improve on one of the 3 parameters our science case is mostly sensitive to, by worsen at the same time one of the others significantly. 

Further \cite{Goldstein:2018bue} performed detailed simulations of the gLSN population using a completely independent technique and pipeline and reached similar conclusions to the ones presented here: rolling cadences are strongly disfavored, and wide-area, long-season surveys with well sampled light curves are optimal.


\begin{table}
\centering
\begin{tabular}{c|c|c|c|c|c}
\multicolumn{1}{c}{}& \multicolumn{2}{c}{\textbf{LSST + follow-up observation}}  & \multicolumn{2}{c}{\textbf{LSST only}} & \multicolumn{1}{c}{total } \\

& total number  & total fraction  & total number  & total fraction  & number \\
&  with & with & with& with & of\\
& good delays & good delays& good delays& good delays&LSNe Ia\\
\hline
kraken\_2044 & 29.5 & 27.2 \% & 6.3 & 5.8 \% & 108.4 \\
\hline
colossus\_2667 & 28.2 & 32.7 \% & 7.3 & 8.4 \% & 86.2 \\
\hline
alt\_sched & 23.7 & 35.3 \% & 8.6 & 12.8 \% & 67.0 \\
\hline
pontus\_2506 & 21.1 & 27.8 \% & 6.4 & 8.4 \% & 75.9 \\
\hline
pontus\_2002 & 21.1 & 21.0 \% & 1.4 & 1.4 \% & 100.0\\
\hline
kraken\_2042 & 20.0 &  25.2 \% & 4.6 & 5.8 \% & 79.3 \\
\hline
kraken\_2026 & 19.9 &  25.8 \% & 3.7 & 4.8 \% & 77.1 \\
\hline
pontus\_2489 & 19.6 &  23.8 \% & 6.1 & 7.4 \% & 82.4 \\
\hline
baseline2018a & 17.4 &  22.4 \% & 2.9 & 3.7 \% & 77.6  \\
\hline
colossus\_2665 & 17.3 & 22.4 \% & 2.9 & 3.7 \% & 77.2  \\
\hline
kraken\_2035 & 16.2 &  21.0 \% & 2.1 & 2.7 \% & 77.0 \\
\hline
colossus\_2664 & 15.8 & 21.0 \% & 2.7 & 4.1 \% & 75.2  \\
\hline
nexus\_2097 & 15.7&  23.8 \% & 3.2 & 4.8 \% & 65.9 \\
\hline
mothra\_2049 & 13.9&  23.8 \% & 2.7 & 4.7 \% & 58.1\\
\hline
alt\_sched\_rolling & 12.7 &  38.7 \% & 5.8 & 16.5 \% & 35.3 \\
\hline
pontus\_2502 & 11.4 &  17.7 \% & 0.9 & 1.4 \% & 64.4 \\
\hline
mothra\_2045 & 11.3& 27.9 \% & 2.5 & 6.1 \% & 40.4\\
\hline
kraken\_2036 & 11.2 &  25.2 \% & 2.1 & 4.7 \% & 44.5 \\


\end{tabular}
\caption{This table quantifies 18 different cadence strategies for measuring time delays in LSNe Ia. The second and third column consider LSST as a discovery machine in combination with follow-up observations in 3 filters (g,r,i) every second night. We assume follow-up observation would start 2 days after the third LSST data point in any filter exceeding the 5-$\sigma$ depth. In the fourth and fifth column only LSST data is used to measure time delays. The sixth column shows the total amount of LSNe Ia (69 \% doubles and 31 \% quads) calculated via \eqref{eq: total number of LSNe Ia from modified OM 10}. The columns "total fraction with good delays" contain the fraction of the  investigated mock LSNe Ia where the time delay could be measured with accuracy better than 1 percent and precision better than 5 percent. We have investigated 100 mock systems for LSST + follow-up and 202 mock systems for the case where only LSST data is used. The columns "total number with good delays" combine the columns "total fraction with good delays" with the column "total number of LSNe Ia". Since for LSST only, the total numbers with good delays are very low, we advocate using LSST as a discovering machine with observational follow up. Therefore the second column is the relevant one to quantify different cadences. For more qualitative conclusions see Table \ref{tab: favoured strategies}}
\label{tab: quantify different observing strategies}
\end{table}
%
\begin{table}
\centering
\begin{tabular}{c|c|c|c}
& good with obs.   &  favored with . \\
& follow-up in addition to LSST  & LSST data only \\
\hline
kraken\_2044 & x & x  \\
\hline
colossus\_2667 & x & x   \\
\hline
alt\_sched & x & x    \\
\hline
pontus\_2506 & x & x \\
\hline
pontus\_2002 & x &    \\
\hline
kraken\_2042  & x &   \\
\hline
kraken\_2026 & x &   \\
\hline
pontus\_2489 & x & x  \\
\hline
baseline2018a & x &      \\
\hline
colossus\_2665 & x &    \\
\hline
kraken\_2035 &  &     \\
\hline
colossus\_2664 & &     \\
\hline
nexus\_2097 &  &   \\
\hline
mothra\_2049 & &   \\
\hline
alt\_sched\_rolling & &x    \\
\hline
pontus\_2502 &&    \\
\hline
mothra\_2045 & &   \\
\hline
kraken\_2036 & &   \\
\end{tabular}
\caption{This table ranks 18 different cadences for the scenarios combining LSST with follow-up and using LSST data only. Cadences marked with a "x" are good for the given scenario, where unmarked ones are disfavored. For our science case of measuring time delays from as many lensed SNe as possible, it would be more effective to use LSST as a discovering machine with additional follow-up, instead of relying on LSST completely for the delay measurements. Therefore we are fine with a "baseline2018a" like cadence. To improve on this cadence longer cumulative seasonal lengths $\bar{t}_\mathrm{eff,cad}$, bigger survey areas $\Omega_\mathrm{cad}$ and a better cadence are helpful. To improve one of these parameters and worsen at the same time one of the others significantly is to be rejected, as in the case of rolling cadences where $\bar{t}_\mathrm{eff,cad}$ is significantly reduced for a better cadence. The key massage is that most of the cadences are ok for our science case, where rolling cadences are clearly disfavored.}
\label{tab: favoured strategies}
\end{table}
%
\FloatBarrier
\subsection{Supernovae Lensed by Galaxy Clusters}
\textit{Contributors: Tanja Petrushevska}

\

Here, we focus on prospects of observing supernovae which are
lensed by known galaxy clusters. High-z galaxies that appear as
multiple images in the cluster field can host supernova
explosions. Strongly lensed supernovae by galaxy clusters not only
can be used as tools to examine both global cosmology, but also
the local environment of the cluster lenses. Cluster lensing time
scales are typically much longer and the microlensing effects are
almost negligible, which makes their measurement potentially more
feasible, especially if the lens potential is well studied and the
predicted time delays have small uncertainties. We calculate the
expected number of supernovae Ia in the multiply lensed background
galaxies by using the Hubble Frontier Fields cluster and Abell
1689. These clusters have been extensively studied, and given the
good quality data, well constrained magnification maps and time
delays can be obtained from the lensing models. We only considered
those that have a spectroscopic redshift. To obtain better image
depth, we combine the images that are taken closer than 5 days in
time. We note that these are a lower limits, since we have only
considered few clusters and the galaxies with spectroscopic
redshift. For this science case, the most important bands are i, z
and y. Since most of the light of nearby SNe is in the optical
bands, these filters are optimal for finding high-z SNe, as their
light is redshifted to the longer wavelengths. When we consider
the different observing strategies, we find that the rolling
cadences (mothra\_2045 and pontus\_2502) are disfavoured given that
they do not return to the clusters as the other observing
strategies. The strategy pontus\_2489 provides slightly better
prospects compared to the others because it provides the most
number of visits to the same cluster fields.

\begin{figure}
\centering
\includegraphics[scale=0.65]{figures/sl_galaxy_lensing.pdf}\caption{The expected total number of strongly lensed SNe Ia arising from the multiply imaged galaxies in the Hubble Frontier Fields and Abell 1689 in function of the observing strategy. \todo{Tanja}{include new cadences and do same estimates for CC SNe*}}
\end{figure}


\subsection{Lensed Quasars}
\textit{Contributors: Phil Marshall, Timo Anguita, Lynne Jones}


The goal of this section is to evaluate the precision we can achieve in measuring time delays in strongly lensed AGN, and as such, the precision on the measurement of the Hubble constant from all systems with measured time delays.

Anticipating that the time delay accuracy would depend on night-to-night
cadence, season length, and campaign length, we carried out a large
scale simulation and measurement program that coarsely sampled these
schedule properties. In \cite{Liao2015}, we simulated 5 different
light curve datasets, each containing 1000 lenses, and presented them to
the strong lensing community in a ``Time Delay Challenge.'' These 5
challenge ``rungs'' differed by their schedule properties. Focusing on the best challenge
submissions made by the community, we derived a simple power law model
for the variation of each of the time delay accuracy, time delay
precision, and useable sample fraction, with the schedule properties
cadence, season length and campaign length. They are
given by the following equations:

\begin{align}
|A|_{\rm model} &\approx 0.06\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{0.0}
\left(\frac{\rm sea}  {\rm 4 months}\right)^{-1.0}
\left(\frac{\rm camp}{\rm 5 years} \right)^{-1.1}  \notag \\
P_{\rm model} &\approx 4.0\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{ 0.7}
\left(\frac{\rm sea}  {\rm 4 months}\right)^{-0.3}
\left(\frac{\rm camp}{\rm 5 years} \right)^{-0.6}  \notag \\
f_{\rm model} &\approx 30\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{-0.4}
\left(\frac{\rm sea}  {\rm 4 months}\right)^{ 0.8}
\left(\frac{\rm camp}{\rm 5 years} \right)^{-0.2} \notag 
\end{align}

All three of these diagnostic metrics would, in an ideal world, be
optimized: this could be achieved by decreasing the night-to-night
cadence (to better sample the light curves), extending the observing
season length (to maximize the chances of capturing a strong variation
and its echo), and extending the campaign length (to increase the number
of effective time delay measurements).

The quantity of greatest scientific interest is the {\it accuracy in
	cosmological parameters}: this could be computed as follows. Setting a
required accuracy threshold  defines the available number of lenses,
which in turns gives us the mean precision per lens there. Combining the
whole sample, we would get the error on the weighted mean time delay, as
used by \cite{Coe2009}. This uncertainty, which scales as one
over the square root of the number of available lenses,  can be roughly
equated to the statistical uncertainty on the Hubble constant.

Our Opsim analysis consists in selecting only the sky survey area that allows time delay measurements with accuracies of $<0.2\%$ \citep{Hojjati2014}. This high accuracy area can be used
to define a ``Gold Sample'' of lenses, whose mean precision per lens we
can compute. The TDC2 useable fraction averaged over this area gives us
the approximate size of this sample: we simply re-scale the 400 lenses
predicted by \cite{Liao2015} by this fraction over the 30\% found
in TDC2. While these numbers are approximate, the ratios between
different observing and analysis strategies provide a useful indication of relative merit.

As described above, we follow \cite{Coe2009} and compute a
very simple time delay distance Figure of Merit ``\texttt{DPrecision}''
as follows. We first combine the fractional time delay precision in
quadrature with an assumed 4\% ``modeling uncertainty,'' and then divide
this by the square root of the number of Gold Sample lenses. This
estimated ensemble distance precision can be straightforwardly related
to cosmological parameter precision, as cite{Coe2009} show
(it's very roughly the precision on the Hubble constant).

Our calculations are performed using the full 10 years of LSST operations with both the single i-band observations and all bands merged together. Naturally, significantly better results are obtained making no distinctions between photometric bands, however, it is important to note that there is a rather large caveat: Even when AGN variability can show almost negligible difference between bands close in wavelength, the difference can be important between the bluest and reddest LSST bands. As such, the i-band results can be interpreted as a very conservative upper limit in the precision attainable and the "all-bands" result as a very optimistic lower limit.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/sl_QSO_Nlens.png}    
		\caption{Number of ``golden lenses'' in sky areas with accuracies $<$0.2\%}   
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{sl_QSO_Precision.png}    
		\caption{Mean precision per lens (percent) in  sky areas with accuracies $<$0.2\%}  
\end{figure}
\begin{figure}
\centering
		\includegraphics[width=\linewidth]{sl_QSO_Dprec.png}    
		\caption{Time delay distance precision figure of merit (percent) in  sky areas with accuracies $<$0.2\%} 
\end{figure}

