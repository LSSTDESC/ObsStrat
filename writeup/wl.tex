\section{Weak Lensing}\label{sec:wl}

\subsection{Systematics in the WFD survey}

The LSST provides an opportunity to mitigate WL systematics using the observing strategy. This opportunity was not possible in previous surveys because the LSST will be the first survey to dither at large scales (relative to the field of view) with a large number of exposures.

We analyzed how different OpSim runs perform with respect to the point-spread function (PSF) modeling errors and similar WL systematics as follows: (a) We create 50 million stars uniformly in the WFD areas of each survey, with cuts based on the coadded depth and dust extinction, consistently the LSS WG. (b) we model the PSF modeling errors at each star in each exposure as a 6\% radial error in the outer 20\% of field of view (and no error). We assume an average relative size error of 0.001 on the PSF model. This model is realistically motivated by recent surveys (e.g. \cite{bosche2018}) (c) We average down the modeling errors across exposures via their second moments, where this operation is linear. You can read more about this process on GitHub \footnote{\url{https://github.com/hsnee/lsstpsf/blob/master/methodology.ipynb}} (d) Propagate the averaged errors into bias on the cosmic shear using the $\rho$-statistics formulation (\cite{rowe2010}; \cite{jarvis2016}). The code to recreate this analysis is available on GitHub \footnote{\url{https://github.com/hsnee/lsstpsf/blob/master/ModelErrorsNew.ipynb}}. 

Figure~\ref{fig:WLSystematicsRankings} shows the result of this analysis, propagated into bias on the cosmic shear signal. 

Table~\ref{table:WLSystematicsRankings} provides a ranking of the strategies. There are certain trends that can be seen from the table (or equivalently, from figure~\ref{fig:WLSystematicsRankings}): more visits in the main WFD survey are very useful for averaging down weak lensing systematics (whether achieved by smaller area, or shorter 20 second visits.) OpSim runs with more exposure to the galactic planes underperform. Runs with rolling cadence significantly underperform others, especially ones that have rolling cadence in all or most years. These are not surprising trends, since prioritizing uniformity in WFD over area increases and rolling strategies is what would essentially drive weak lensing systematics down. Table~\ref{table:WLSystematicsRankings} also shows the average number of exposures in $i$-band for 5000 objects randomly drawn from a uniform distribution throughout the areas covered by each of the OpSim runs, and shows a strong correlation between this number and the performance of the runs. Rolling cadence runs and wider area runs also appear to have significant number of average visits. We are also exploring additional explanations of why rolling cadence runs are consistently underperforming.


\begin{figure}[htb]
    \centering
    \caption{The bias on the cosmic shear signal as a function of angular separation for all recent OpSim runs, at years 1, 3, 6, and 10. This indicates their performance with respect to weak lensing systematics, with higher curves corresponding to higher bias/worse performance.}
    \label{fig:WLSystematicsRankings}
\includegraphics[width=0.72\textwidth]{figures/WLSystematicsAllYears.png}
\end{figure}




\begin{table}[ht]
\caption{A ranking of the recent observing strategies with respect to weak lensing systematics, as
  explained in the text, for years 1 and 10. Repeated numbers are ties. $\langle N\rangle_i^{(Y10)}$
  is the number of observations in $i$-band for objects on average after 10 years -- a simple metric
  that strongly correlates with the performance of an OpSim run, $\langle N\rangle_i^{(Y10)}$ is typically higher for better-performing runs.}
\begin{tabular}{lllll}
\label{table:WLSystematicsRankings}
OpSim run & Y1 & Y10 & $\langle N\rangle_i^{(Y10)}$ \\ \hline
mothra\_2045   & {\color{orange}9} & {\color{red}14}    & 134 \\
kraken\_2044   & {\color{red}11}   & {\color{red}13}    & 162 \\
nexus\_2097    & {\color{red}14}   & {\color{red}12}    & 159 \\
pontus\_2002   & {\color{red}12}   & {\color{red}11}    & 160 \\
kraken\_2036   & {\color{yellow}6} & {\color{orange}10} & 178 \\
pontus\_2502   & {\color{red} 12}  & {\color{orange}9}  & 179 \\
kraken\_2035   & {\color{green}2}  & {\color{yellow}8}  & 210 \\
colossus\_2664 & {\color{yellow}6} & {\color{yellow}7}  & 208 \\
kraken\_2026   & {\color{green}2}  & {\color{yellow}5}  & 217 \\
baseline2018a  & {\color{green}2}  & {\color{yellow}5}  & 212 \\
colossus\_2665 & {\color{orange}9} & {\color{green}3}   & 210 \\ 
colossus\_2667 & {\color{green}2}  & {\color{green}3}   & 220 \\
kraken\_2042   & {\color{orange}8} & {\color{green}2}   & 234 \\
pontus\_2489   & {\color{green}1}  & {\color{green}1}   & 306

\end{tabular}
\end{table}

There are several metrics that contain similar information to that in our analysis in MAF: the distribution of angles pointing to the center of the field of view in each observation \footnote{\url{https://github.com/LSST-nonproject/sims\_maf\_contrib/blob/5fd4ad762509cab0b4e403d84f2e65ed1ad08c06/science/static/WeakLensing\_Focal\_Angle.ipynb}} from Peter Yoachim which returns the Kolmogorov-Smirnov test's D-statistic for this distribution of angles against a uniform distribution, the distribution of coadded-depth (high values and narrow distribution is best) the distribution of airmass (low values and narrow distribution is best), the distribution of seeing values (also low values and narrow distribution is best). The reason these metrics provide similar results to our analysis is that survey uniformity is one of the crucial things that averages down WL systematics, and all these metrics are, in one way or another, a measure of uniformity.


\subsection{WL needs for DDFs}

For the DDFs to serve as a ``noise free'' training sample for shear estimation methods that need
such a training sample, and to provide injection templates for Balrog or other injection methods,
the DDFs should ideally be at least 40 deg$^2$ total across all DDFs, with a total effective
exposure time (``effective'' meaning open-shutter time factoring in transparency and sky brightness) 
exceeding that in the WFD survey by at least a factor of 10 in all bands.  In addition it is
important that the DDF effective exposure time should be >10x the WFD value for any chosen upper bound
of seeing FWHM, at least in r and i bands but ideally all bands.  The science driver for this
requirement for WL is as follows.  First, some state-of-the-art methods for inferring WL shear
\citep{2014MNRAS.438.1880B,2014MNRAS.438.1880B} use a data-driven prior on the distribution of
the first, second, and higher order moments of galaxy light profiles.  Second, characterizing and
understanding the impact of blending on both shear and photometric redshifts will likely rely at
least at some level on injection simulations -- i.e. the insertion of fake objects based on
DDF-driven templates for galaxy morphologies and colors into real WFD data -- as illustrated in
e.g. \citet{2016MNRAS.457..786S,2018PASJ...70S...6H}.  Injection simulations in particular require
multi-band deep templates that have at least as good seeing as the WFD survey and at least 10x the
effective exposure time.

Overlap with external deep drilling fields from e.g. WFIRST and Euclid will be used to improve
photometric redshifts, test shear calibration, and improve and test deblending. Overlap with CMB
data can be used for cross-calibration of systematics in LSST weak lensing. 

The current 4 deep fields work well for WL.  For additional fields, priority should be given to
those with NIR (Euclid deep, WFIRST, JWST), other deep multi-band photometry, very deep
spectroscopy (JWST or 30m class telescopes), or deep high-resolution CMB data.

Euclid NIR is too shallow to solve LSST photo-z calibration; overlap with Euclid deep fields is
highly desirable (especially since Euclid wide will help with blending but not to the depths of the
samples we can use for LSST). WFIRST overlap with DDFs is critical; it does not need to be cadenced.  Ideally it would also overlap with
deep X-ray data and deep high-resolution CMB data.

